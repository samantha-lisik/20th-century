**20th Century Events Web Scraping Project**

This project focuses on web scraping and data collection as part of the CareerFoundry Data Analytics Program.
The goal was to scrape historical event data from a public web page and prepare it for further analysis and network visualization.

**Project overview**

In this exercise, I scraped content from the “Key Events of the 20th Century” Wikipedia web page to collect structured data that can later be used for analytical exploration and network analysis.

**The project demonstrates**:

- Setting up and using a dedicated Python virtual environment

- Scraping web content using automation tools

- Saving scraped data in a reusable format

- Working within a reproducible data science workflow

**Data source**:

Public Wikipedia page: Key Events of the 20th Century: https://en.wikipedia.org/wiki/Key_events_of_the_20th_century

The full page was downloaded and scraped in accordance with the CareerFoundry project brief.

No proprietary or restricted data was used.

**Tools & technologies used**:

- Python

- JupyterLab

- Conda virtual environment (20th_century)

- Selenium

- ChromeDriver

- BeautifulSoup

- requirements.txt for dependency management

**Workflow summary**:

1. Activated the 20th_century virtual environment using Conda

2. Launched JupyterLab with the appropriate kernel

3. Installed required libraries using requirements.txt

4. Set up ChromeDriver for automated browser interaction

5. Scraped the full content of the Key Events of the 20th Century page

6. Saved the scraped content as a .txt file in the working directory

7. Pushed the notebook and output file to GitHub

**Output**

- A text file containing the scraped page content

- A Jupyter notebook documenting the scraping logic and process

- These outputs serve as the foundation for later network visualization and analysis tasks.

**Notes**

- The data was scraped for educational purposes only

- This project focuses on data collection rather than analysis or visualization
